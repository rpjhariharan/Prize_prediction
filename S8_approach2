Linear Regression

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predictions
y_train_pred = lr_model.predict(X_train)
y_test_pred = lr_model.predict(X_test)

# Calculating performance metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
test_rmse = mean_squared_error(y_test, y_test_pred)

# Cross-validation for R² score
cv_r2_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')
cv_mean_r2 = np.mean(cv_r2_scores)
cv_std_r2 = np.std(cv_r2_scores)

# Displaying results
print("Train MAE:", train_mae)
print("Test MAE:", test_mae)
print("Train R²:", train_r2)
print("Test R²:", test_r2)
print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)
print("CV Mean R²:", cv_mean_r2)
print("CV Std R²:", cv_std_r2) 

Train MAE: 179,248.30
Test MAE: 185,853.67
Train R²: 0.6052
Test R²: 0.5900
Train RMSE: 225,509.96
Test RMSE: 232,127.36
CV Mean R²: 0.6040
CV Std R²: 0.0151

Interpreting the Metrics
Mean Absolute Error (MAE):

Train MAE: 179,248
Test MAE: 185,853
MAE shows the average absolute error between the predicted and actual prices. These values indicate that, on average, the model’s predictions are off by around 180,000, which may be high depending on the range of prices in your dataset. Lower MAE values would suggest more precise predictions.
R² (Coefficient of Determination):

Train R²: 0.605
Test R²: 0.590
These values mean that the model explains about 60% of the variance in car prices, which indicates it captures some but not all of the factors influencing price. Higher R² values (closer to 1) generally indicate a better fit, so this could likely be improved with more complex models or additional features.
Root Mean Squared Error (RMSE):

Train RMSE: 225,509
Test RMSE: 232,127
RMSE penalizes larger errors more heavily than MAE and is often used to understand prediction accuracy. The high RMSE values here suggest there is significant room for improvement in prediction accuracy.
Cross-Validation (CV Mean R² and Std R²):

CV Mean R²: 0.604
CV Std R²: 0.015
The consistent cross-validation score across folds shows that the model’s performance is stable, but a higher mean R² would indicate it is better at capturing patterns in unseen data.

2. Decision tree

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Decision Tree model
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Predictions
y_train_pred = dt_model.predict(X_train)
y_test_pred = dt_model.predict(X_test)

# Calculating performance metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)

# Cross-validation for R² score
cv_r2_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='r2')
cv_mean_r2 = np.mean(cv_r2_scores)
cv_std_r2 = np.std(cv_r2_scores)

# Displaying results
print("Train MAE:", train_mae)
print("Test MAE:", test_mae)
print("Train R²:", train_r2)
print("Test R²:", test_r2)
print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)
print("CV Mean R²:", cv_mean_r2)
print("CV Std R²:", cv_std_r2)


Train MAE: 5051.359650904746
Test MAE: 127824.52388172044
Train R²: 0.9902653929430314
Test R²: 0.6340515629044253
Train RMSE: 35347.323678098364
Test RMSE: 219428.6458665552
CV Mean R²: 0.6173505283434202
CV Std R²: 0.034719308676002654

The Decision Tree results show mixed performance, with signs of overfitting due to the large gap between training and test metrics:

High Train R² (0.9898) and very low Train MAE (5,383.34) indicate that the model fits the training data exceptionally well.
However, the Test R² (0.6378) and Test MAE (125,400.69) suggest that the model doesn’t generalize as effectively on unseen data.
This overfitting is common in Decision Trees when left unrestricted because they can easily memorize the training data. The Cross-Validation R² (0.6544), though slightly higher than Linear Regression, further supports the overfitting diagnosis, as the model struggles with generalization despite capturing complex relationships.

3. Random forest

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)


# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Random Forest model
rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred_rf = rf_model.predict(X_train)
y_test_pred_rf = rf_model.predict(X_test)

# Calculating performance metrics
train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)
test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)
train_r2_rf = r2_score(y_train, y_train_pred_rf)
test_r2_rf = r2_score(y_test, y_test_pred_rf)
train_rmse_rf = mean_squared_error(y_train, y_train_pred_rf, squared=False)
test_rmse_rf = mean_squared_error(y_test, y_test_pred_rf, squared=False)

# Cross-validation for R² score
cv_r2_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)
cv_mean_r2_rf = np.mean(cv_r2_scores_rf)
cv_std_r2_rf = np.std(cv_r2_scores_rf)

# Displaying results
print("Train MAE:", train_mae_rf)
print("Test MAE:", test_mae_rf)
print("Train R²:", train_r2_rf)
print("Test R²:", test_r2_rf)
print("Train RMSE:", train_rmse_rf)
print("Test RMSE:", test_rmse_rf)
print("CV Mean R²:", cv_mean_r2_rf)
print("CV Std R²:", cv_std_r2_rf)

Train MAE: 42389.60772165776
Test MAE: 110196.78705084835
Train R²: 0.9616154961568181
Test R²: 0.7687921389287078
Train RMSE: 70190.00762642935
Test RMSE: 174415.33749719514
CV Mean R²: 0.7613061154958695
CV Std R²: 0.02010882013940389

4. Gradient Boosting

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV for Gradient Boosting
param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'min_samples_split': [5, 10]
}

# Initialize GradientBoostingRegressor
gb_model = GradientBoostingRegressor(random_state=42)

# Set up GridSearchCV for Gradient Boosting
grid_search_gb = GridSearchCV(estimator=gb_model, param_grid=param_grid_gb, cv=5, scoring='r2', n_jobs=-1)
grid_search_gb.fit(X_train, y_train)

# Display best parameters from GridSearchCV
print("Best Parameters:", grid_search_gb.best_params_)

# Retrain the model with the best parameters
best_gb_model = grid_search_gb.best_estimator_
best_gb_model.fit(X_train, y_train)

# Predictions with the optimized Gradient Boosting model
y_train_pred_gb = best_gb_model.predict(X_train)
y_test_pred_gb = best_gb_model.predict(X_test)

# Calculate performance metrics
train_mae_gb = mean_absolute_error(y_train, y_train_pred_gb)
test_mae_gb = mean_absolute_error(y_test, y_test_pred_gb)
train_r2_gb = r2_score(y_train, y_train_pred_gb)
test_r2_gb = r2_score(y_test, y_test_pred_gb)
train_rmse_gb = mean_squared_error(y_train, y_train_pred_gb, squared=False)
test_rmse_gb = mean_squared_error(y_test, y_test_pred_gb, squared=False)

# Cross-validation for R² score
cv_r2_scores_gb = cross_val_score(best_gb_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)
cv_mean_r2_gb = np.mean(cv_r2_scores_gb)
cv_std_r2_gb = np.std(cv_r2_scores_gb)

# Displaying results
print("Train MAE:", train_mae_gb)
print("Test MAE:", test_mae_gb)
print("Train R²:", train_r2_gb)
print("Test R²:", test_r2_gb)
print("Train RMSE:", train_rmse_gb)
print("Test RMSE:", test_rmse_gb)
print("CV Mean R²:", cv_mean_r2_gb)
print("CV Std R²:", cv_std_r2_gb)


Train MAE: 48050.34717071208
Test MAE: 95429.10807847201
Train R²: 0.9554474545789445
Test R²: 0.8210719415548611
Train RMSE: 75751.69315008793
Test RMSE: 153354.62606062644
CV Mean R²: 0.8248087275674415
CV Std R²: 0.013167844790359224

5. XGBoost

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)
# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV for XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# Initialize XGBRegressor
xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')

# Set up GridSearchCV for XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=5, scoring='r2', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Display best parameters from GridSearchCV
print("Best Parameters:", grid_search_xgb.best_params_)

# Retrain the model with the best parameters
best_xgb_model = grid_search_xgb.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Predictions with the optimized XGBoost model
y_train_pred_xgb = best_xgb_model.predict(X_train)
y_test_pred_xgb = best_xgb_model.predict(X_test)

# Calculate performance metrics
train_mae_xgb = mean_absolute_error(y_train, y_train_pred_xgb)
test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)
train_r2_xgb = r2_score(y_train, y_train_pred_xgb)
test_r2_xgb = r2_score(y_test, y_test_pred_xgb)
train_rmse_xgb = mean_squared_error(y_train, y_train_pred_xgb, squared=False)
test_rmse_xgb = mean_squared_error(y_test, y_test_pred_xgb, squared=False)

# Cross-validation for R² score
cv_r2_scores_xgb = cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)
cv_mean_r2_xgb = np.mean(cv_r2_scores_xgb)
cv_std_r2_xgb = np.std(cv_r2_scores_xgb)

# Displaying results
print("Train MAE:", train_mae_xgb)
print("Test MAE:", test_mae_xgb)
print("Train R²:", train_r2_xgb)
print("Test R²:", test_r2_xgb)
print("Train RMSE:", train_rmse_xgb)
print("Test RMSE:", test_rmse_xgb)
print("CV Mean R²:", cv_mean_r2_xgb)
print("CV Std R²:", cv_std_r2_xgb)

Train MAE: 74233.67417792196
Test MAE: 108126.59710857227
Train R²: 0.9008272843753917
Test R²: 0.7831456028614576
Train RMSE: 113019.18893191472
Test RMSE: 168826.94936002727
CV Mean R²: 0.780870774791353
CV Std R²: 0.02091082306271525
