import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Cleaned_and_Adjusted_Car_Dataset.csv'
car_data = pd.read_csv(file_path)
# Selecting the relevant features and target variable
X = car_data[['Kilometers Driven', 'Car Age', 'Mileage', 'Previous Owners']]
y = car_data['Adjusted Price']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV for XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# Initialize XGBRegressor
xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')

# Set up GridSearchCV for XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=5, scoring='r2', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Display best parameters from GridSearchCV
print("Best Parameters:", grid_search_xgb.best_params_)

# Retrain the model with the best parameters
best_xgb_model = grid_search_xgb.best_estimator_
best_xgb_model.fit(X_train, y_train)

# Predictions with the optimized XGBoost model
y_train_pred_xgb = best_xgb_model.predict(X_train)
y_test_pred_xgb = best_xgb_model.predict(X_test)

# Calculate performance metrics
train_mae_xgb = mean_absolute_error(y_train, y_train_pred_xgb)
test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)
train_r2_xgb = r2_score(y_train, y_train_pred_xgb)
test_r2_xgb = r2_score(y_test, y_test_pred_xgb)
train_rmse_xgb = mean_squared_error(y_train, y_train_pred_xgb, squared=False)
test_rmse_xgb = mean_squared_error(y_test, y_test_pred_xgb, squared=False)

# Cross-validation for RÂ² score
cv_r2_scores_xgb = cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)
cv_mean_r2_xgb = np.mean(cv_r2_scores_xgb)
cv_std_r2_xgb = np.std(cv_r2_scores_xgb)

# Displaying results
print("Train MAE:", train_mae_xgb)
print("Test MAE:", test_mae_xgb)
print("Train RÂ²:", train_r2_xgb)
print("Test RÂ²:", test_r2_xgb)
print("Train RMSE:", train_rmse_xgb)
print("Test RMSE:", test_rmse_xgb)
print("CV Mean RÂ²:", cv_mean_r2_xgb)
print("CV Std RÂ²:", cv_std_r2_xgb)


Analysis and Selection of the Best Model
Linear Regression: Baseline model with lower scores in both train and test 
ğ‘…
2
R 
2
  values, showing limited predictive capability compared to more complex models.

Decision Tree: Shows signs of overfitting, with an extremely high train 
ğ‘…
2
R 
2
  (0.99) but a lower test 
ğ‘…
2
R 
2
  (0.634). The variance between CV scores is relatively high, indicating instability across different subsets.

Random Forest: Significant improvement over Decision Tree, with good test 
ğ‘…
2
R 
2
  (0.7688) and a high CV Mean 
ğ‘…
2
R 
2
  (0.7613), but the Train 
ğ‘…
2
R 
2
  (0.9616) and Train RMSE indicate a slight overfitting tendency.

Gradient Boosting: Best generalization performance among the models, with a high Test 
ğ‘…
2
R 
2
  (0.8211), the lowest Test MAE (95,429.11), and the highest CV Mean 
ğ‘…
2
R 
2
  (0.8248) with a low standard deviation (0.0132). These results indicate good predictive power and stability.

XGBoost: Comparable to Random Forest but slightly behind Gradient Boosting in terms of generalization. While it achieves a decent Test 
ğ‘…
2
R 
2
  (0.7831), it has a higher Train MAE and Test MAE than Gradient Boosting.

Conclusion
Gradient Boosting is the best model for this dataset based on the following:

It achieves the highest test 
ğ‘…
2
R 
2
  (0.8211) and lowest Test MAE.
The Cross-Validation Mean 
ğ‘…
2
R 
2
  is the highest (0.8248) with a low standard deviation, showing stable performance across folds.
The metrics indicate good balance and generalization, without significant overfitting.
