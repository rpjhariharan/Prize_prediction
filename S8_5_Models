1. Linear Regression

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score

# Load Dataset
file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Feature_Engineered_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Define Features and Target
X = car_data.drop(columns=['Price'])
y = car_data['Price']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict
y_train_pred = linear_model.predict(X_train)
y_test_pred = linear_model.predict(X_test)

# Metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(linear_model, X, y, cv=5, scoring='r2')

# Results
print("Linear Regression Results:")
print("Train MAE:", train_mae, "Test MAE:", test_mae)
print("Train R²:", train_r2, "Test R²:", test_r2)
print("Train RMSE:", train_rmse, "Test RMSE:", test_rmse)
print("CV Mean R²:", cv_scores.mean(), "CV Std R²:", cv_scores.std()) 


Linear Regression Results:
Train MAE: 271237.845089428      Test MAE: 273374.8420486624
Train R²: 0.743014054401487      Test R²: 0.7028211340655481
Train RMSE: 538115.8772033883    Test RMSE: 573596.1074647482
CV Mean R²: 0.6941358364545447   CV Std R²: 0.039414358227388764



2. Decision Tree Regressor

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score

# Load Dataset
file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Feature_Engineered_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Define Features and Target
X = car_data.drop(columns=['Price'])
y = car_data['Price']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Model
tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(X_train, y_train)

# Predict
y_train_pred = tree_model.predict(X_train)
y_test_pred = tree_model.predict(X_test)

# Metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(tree_model, X, y, cv=5, scoring='r2')

# Results
print("Decision Tree Results:")
print("Train MAE:", train_mae, "Test MAE:", test_mae)
print("Train R²:", train_r2, "Test R²:", test_r2)
print("Train RMSE:", train_rmse, "Test RMSE:", test_rmse)
print("CV Mean R²:", cv_scores.mean(), "CV Std R²:", cv_scores.std()) 

Decision Tree Results:
Train MAE: 0.0                      Test MAE: 179867.39967702868
Train R²: 1.0                       Test R²: 0.7549039801273715
Train RMSE: 0.0                     Test RMSE: 520913.20201449527
CV Mean R²: 0.691670645617282       CV Std R²: 0.10400235876337932





3. Random Forest Regressor

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score

# Load Dataset
file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Feature_Engineered_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Define Features and Target
X = car_data.drop(columns=['Price'])
y = car_data['Price']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Model
forest_model = RandomForestRegressor(random_state=42)
forest_model.fit(X_train, y_train)

# Predict
y_train_pred = forest_model.predict(X_train)
y_test_pred = forest_model.predict(X_test)

# Metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(forest_model, X, y, cv=5, scoring='r2')

# Results
print("Random Forest Results:")
print("Train MAE:", train_mae, "Test MAE:", test_mae)
print("Train R²:", train_r2, "Test R²:", test_r2)
print("Train RMSE:", train_rmse, "Test RMSE:", test_rmse)
print("CV Mean R²:", cv_scores.mean(), "CV Std R²:", cv_scores.std())

Random Forest Results:
Train MAE: 50969.998942357626               Test MAE: 136235.44641098104
Train R²: 0.9813511978740873                Test R²: 0.8478950291364753
Train RMSE: 144959.46926353726              Test RMSE: 410363.7015327563
CV Mean R²: 0.8514334268892239              CV Std R²: 0.03370134739953576





4. Gradient Boosting Regressor
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score


# Load Dataset
file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Feature_Engineered_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Define Features and Target
X = car_data.drop(columns=['Price'])
y = car_data['Price']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Model
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)

# Predict
y_train_pred = gb_model.predict(X_train)
y_test_pred = gb_model.predict(X_test)

# Metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='r2')

# Results
print("Gradient Boosting Results:")
print("Train MAE:", train_mae, "Test MAE:", test_mae)
print("Train R²:", train_r2, "Test R²:", test_r2)
print("Train RMSE:", train_rmse, "Test RMSE:", test_rmse)
print("CV Mean R²:", cv_scores.mean(), "CV Std R²:", cv_scores.std())


Gradient Boosting Results:
Train MAE: 139694.8432585323                Test MAE: 162177.94368510792
Train R²: 0.9324920457772498                Test R²: 0.8489810280225152
Train RMSE: 275802.7508089989               Test RMSE: 408896.12015263166
CV Mean R²: 0.8359672394265608              CV Std R²: 0.030725204042089543




5. XGBoost Regressor
import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score


# Load Dataset
file_path = r'D:\OneDrive\Desktop\Car\Cleaned\Feature_Engineered_Car_Dataset.csv'
car_data = pd.read_csv(file_path)

# Define Features and Target
X = car_data.drop(columns=['Price'])
y = car_data['Price']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Model
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)

# Predict
y_train_pred = xgb_model.predict(X_train)
y_test_pred = xgb_model.predict(X_test)

# Metrics
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')

# Results
print("XGBoost Regressor Results:")
print("Train MAE:", train_mae, "Test MAE:", test_mae)
print("Train R²:", train_r2, "Test R²:", test_r2)
print("Train RMSE:", train_rmse, "Test RMSE:", test_rmse)
print("CV Mean R²:", cv_scores.mean(), "CV Std R²:", cv_scores.std())

XGBoost Regressor Results:
Train MAE: 47603.98596848561               Test MAE: 129154.55403777251
Train R²: 0.9958639873136551               Test R²: 0.8498139627582278
Train RMSE: 68267.19377968367              Test RMSE: 407766.9418333314
CV Mean R²: 0.8549876704274961             CV Std R²: 0.029727135900826197




XGBoost Regressor has the highest accuracy with:

Test MAE: 129154.55403777251
Test R²: 0.8498139627582278
Test RMSE: 407766.9418333314
CV Mean R²: 0.8549876704274961 
CV Std R²: 0.029727135900826197
This makes XGBoost the most reliable model for price prediction.
